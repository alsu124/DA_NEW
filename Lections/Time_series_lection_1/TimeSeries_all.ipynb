{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series in Python\n",
    "# Practical approach\n",
    "\n",
    "Автор: Сергеев Дмитрий\n",
    "\n",
    "https://github.com/DmitrySerg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## План\n",
    "1. Введение (~15 мин)\n",
    "  * Основные определения, варианты постановки задачи\n",
    "  * Метрики качества прогнозов, R^2, MSE, MAE, MAPE и т.д.\n",
    "2. Движемся, сглаживаем и оцениваем (~40 мин)\n",
    "  * Rolling window estimations + практика, прогнозируем оконными функциями, считаем качество\n",
    "  * Экспоненциальное сглаживание, простое, двойное и Holt-Winters + примеры с ручной настройкой параметров\n",
    "  * Кросс-валидация на временных рядах, подбор параметров + практика, оценка Holt-Winters с минимизацией ошибки на кросс-валидации на разных рядах\n",
    "3. Эконометрический подход (~50 мин)\n",
    "  * Стационарность и единичные корни - интуиция теории + практика проверки разных рядов на стационарность, ADF\n",
    "  * Избавление от нестационарности, дифференцирование, сезонное дифференцирование, выделение трендов + практика, приводим ряд к стационарному виду\n",
    "  * ARIMA, немного интуиции, выбор начальных параметров модели по ACF, PACF, выбор параметров перебором, построение SARIMAX с подобранными параметрами\n",
    "  * Ограничения, недостатки эконометрического подхода\n",
    "4. Machine Learning для временных рядов (~50 мин)\n",
    "  * Feature extraction, engineering, предобработка данных, какие признаки и как можно извлечь + практика, формируем датасет для моделей по ряду, пишем функцию для автогенерации признаков\n",
    "  * Линейные модели, обучаем, смотрим качество на кросс-валидации, оцениваем важность признаков\n",
    "  * Регуляризация, отбор признаков, сокращение признакового пространства, ridge, lasso\n",
    "  * Бустиг для временных рядов - ограничения и преимущества\n",
    "5. To Infinity and Beyond (~20 мин)\n",
    "  * Facebook Prophet - автоматические прогнозы\n",
    "  * Что еще можно делать с временными рядами - RNN, LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np                               # работа с векторами и матрицами\n",
    "import pandas as pd                              # таблицы и операции над данными\n",
    "import matplotlib.pyplot as plt                  # графики\n",
    "import seaborn as sns                            # еще графики\n",
    "\n",
    "from dateutil.relativedelta import relativedelta # для комфортной работы с датами\n",
    "from scipy.optimize import minimize              # позволяет минимизировать функции\n",
    "\n",
    "import statsmodels.formula.api as smf            # всякая статистика и эконометрика\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "\n",
    "from itertools import product                    # немножко функций для удобства\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import warnings                                  # чтобы никто не мешал бесчинствам с кодом\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На работе я практически ежедневно сталкиваюсь с теми или иными задачами, связанными с временными рядам. Чаще всего возникает вопрос — а что у нас будет происходить с нашими показателями в ближайший день/неделю/месяц/пр. — сколько игроков установят приложения, сколько будет онлайна, как много действий совершат пользователи, и так далее. К задаче прогнозирования можно подходить по-разному, в зависимости от того, какого качества должен быть проноз, на какой период мы хотим его строить, и, конечно, как долго нужно подбирать и настраивать параметры модели для его получения.\n",
    "\n",
    "Небольшое [определение](https://ru.wikipedia.org/wiki/Временной_ряд) временного ряда:\n",
    "> Временной ряд – это последовательность значений, описывающих протекающий во времени процесс, измеренных в последовательные моменты времени, обычно через равные промежутки\n",
    "\n",
    "Таким образом, данные оказываются упорядочены относительно неслучайных моментов времени, и, значит, в отличие от случайных выборок, могут содержать в себе дополнительную информацию, которую мы постараемся извлечь. \n",
    "\n",
    "В целом, задачи, связанные с временными рядами можно разбить на несколько групп\n",
    "- Прогнозирование - когда хотим узнать, что будет дальше\n",
    "- Поиск аномалий - когда хотим понять, где были проблемы в прошлом\n",
    "- Кластеризация и классификация - когда временные ряды сами являются признаками объектов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C какими временными рядами будем работать\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ads = pd.read_csv('data/ads.csv', index_col=['Time'], parse_dates=['Time'])\n",
    "currency = pd.read_csv('data/currency.csv', index_col=['Time'], parse_dates=['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(ads.Ads)\n",
    "plt.title('Просмотры рекламы')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(currency.GEMS_GEMS_SPENT)\n",
    "plt.title('Траты внутриигровой валюты')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики качества прогноза\n",
    "\n",
    "Рассмотрим основные и самые распространенные метрики качества прогнозов, которые, по большому счету, являются метриками для задачи регрессии и используются далеко не только во временных рядах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [R squared](http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score-the-coefficient-of-determination), коэффициент детерминации (в эконометрике - доля объясненной моделью дисперсии), $(-\\infty, 1]$\n",
    "\n",
    "$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$ \n",
    "\n",
    "```python\n",
    "sklearn.metrics.r2_score\n",
    "```\n",
    "---\n",
    "- [Mean Absolute Error](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error), интерпретируемая метрика, измеряется в тех же единицах, что и исходный ряд, $[0, +\\infty)$\n",
    "\n",
    "$MAE = \\frac{\\sum\\limits_{i=1}^{n} |y_i - \\hat{y}_i|}{n}$ \n",
    "\n",
    "```python\n",
    "sklearn.metrics.mean_absolute_error\n",
    "```\n",
    "---\n",
    "- [Median Absolute Error](http://scikit-learn.org/stable/modules/model_evaluation.html#median-absolute-error), также интерпретируемая метрика, однако её преимущество - нечувствительность (робастность) к выбросам в данных, $[0, +\\infty)$\n",
    "\n",
    "$MedAE = median(|y_1 - \\hat{y}_1|, ... , |y_n - \\hat{y}_n|)$\n",
    "\n",
    "```python\n",
    "sklearn.metrics.median_absolute_error\n",
    "```\n",
    "---\n",
    "- [Mean Squared Error](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error), используется в большинстве случаев, сильнее наказывает модель за большие ошибки и меньше - за маленькие (парабола), $[0, +\\infty)$\n",
    "\n",
    "$MSE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "```python\n",
    "sklearn.metrics.mean_squared_error\n",
    "```\n",
    "---\n",
    "- [Mean Squared Logarithmic Error](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-logarithmic-error), практически тоже самое, но значения предварительно логарифмируются, таким образом маленьким ошибкам также уделяется значительное внимание, обычно используется, если данным присущ экспоненциальный рост, $[0, +\\infty)$\n",
    "\n",
    "$MSLE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (log(1+y_i) - log(1+\\hat{y}_i))^2$\n",
    "\n",
    "```python\n",
    "sklearn.metrics.mean_squared_log_error\n",
    "```\n",
    "---\n",
    "- Mean Absolute Percentage Error, как MAE, только в процентах, - удобно для объяснения заказчику качества прогноза, $[0, +\\infty)$\n",
    "\n",
    "$MAPE = \\frac{100}{n}\\sum\\limits_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{y_i}$ \n",
    "\n",
    "```python\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# импортируем всё, о чем говорили выше\n",
    "\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, теперь мы знаем, как можно измерить качество нашего прогноза, какие метрики стоит применять и как объяснить содеянное заказчику, дело осталось за малым - нужно построить прогноз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Движемся, сглаживаем и оцениваем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling window estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем моделирование с наивного предположения - \"завтра будет, как вчера\", но вместо модели вида $\\hat{y}_{t} = y_{t-1}$ будем считать, что будущее значение переменной зависит от среднего $n$ её предыдущих значений, а значит, воспользуемся скользящей средней. \n",
    "\n",
    "$\\hat{y}_{t} = \\frac{1}{k} \\displaystyle\\sum^{k-1}_{n=0} y_{t-n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moving_average(series, n):\n",
    "    \"\"\"\n",
    "        Calculate average of last n observations\n",
    "    \"\"\"\n",
    "    return np.average(series[-n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moving_average(ads, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, такой прогноз долгосрочным сделать не удастся, для получения предсказания на шаг вперед предыдущее значение должно быть фактически наблюдаемой величиной. Зато у скользящей средней есть другое применение - сглаживание исходного ряда для выявления трендов, в пандасе есть готовая реализация - [`DataFrame.rolling(window).mean()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html). Чем больше зададим ширину интервала - тем более сглаженным окажется тренд. В случае, если данные сильно зашумлены, что особенно часто встречается, например, в финансовых показателях, такая процедура может помочь увидеть общие паттерны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим такое сглаживание к нашим рядам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotMovingAverage(series, window, plot_intervals=False, scale=1.96, plot_anomalies=False):\n",
    "\n",
    "    \"\"\"\n",
    "        series - dataframe with timeseries\n",
    "        window - rolling window size \n",
    "        plot_intervals - show confidence intervals\n",
    "        plot_anomalies - show anomalies \n",
    "\n",
    "    \"\"\"\n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.title(\"Moving average\\n window size = {}\".format(window))\n",
    "    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n",
    "\n",
    "    # При желании, можно строить и доверительные интервалы для сглаженных значений\n",
    "    if plot_intervals:\n",
    "        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n",
    "        deviation = np.std(series[window:] - rolling_mean[window:])\n",
    "        lower_bond = rolling_mean - (mae + scale * deviation)\n",
    "        upper_bond = rolling_mean + (mae + scale * deviation)\n",
    "        plt.plot(upper_bond, \"r--\", label=\"Upper Bond / Lower Bond\")\n",
    "        plt.plot(lower_bond, \"r--\")\n",
    "        \n",
    "        # а имея доверительные интервалы, можно искать аномальные значения\n",
    "        if plot_anomalies:\n",
    "            anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n",
    "            anomalies[series<lower_bond] = series[series<lower_bond]\n",
    "            anomalies[series>upper_bond] = series[series>upper_bond]\n",
    "            plt.plot(anomalies, \"ro\", markersize=10)\n",
    "        \n",
    "    plt.plot(series[window:], label=\"Actual values\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сглаживание по последним 4 часам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotMovingAverage(ads, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По 12 часам (полдня)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotMovingAverage(ads, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сглаживание по 24 часам (одни сутки) - выделяется дневной тренд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotMovingAverage(ads, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно отобразить доверительные интервалы для наших средних значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotMovingAverage(ads, 4, plot_intervals=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь попробуем при помощи этих доверительных интервалов поймать аномалию во временном ряде. К сожалению или к счастью, конкретно в этом ряду всё более-менее хорошо и гладко, поэтому аномальное значение мы зададим самостоятельно в датафрейме `ads_anomaly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ads_anomaly = ads.copy()\n",
    "ads_anomaly.iloc[-20] = ads_anomaly.iloc[-20] * 0.2 # падение показателя на 80% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И посмотрим, удастся ли таким простым образом её поймать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotMovingAverage(ads_anomaly, 4, plot_intervals=True, plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тоже самое можно проделать и со вторым рядом, попробуем сгладить его с окном в 7 дней, то есть по неделям, и поймать аномалии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotMovingAverage(currency, 7, plot_intervals=True, plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модификацией простой скользящей средней является взвешенная средняя, внутри которой наблюдениям придаются различные веса, в суме дающие единицу, при этом обычно последним наблюдениям присваивается больший вес. \n",
    "\n",
    "\n",
    "$\\hat{y}_{t} = \\displaystyle\\sum^{k}_{n=1} \\omega_n y_{t+1-n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_average(series, weights):\n",
    "    \"\"\"\n",
    "        Calculate weighter average on series\n",
    "    \"\"\"\n",
    "    result = 0.0\n",
    "    weights.reverse()\n",
    "    for n in range(len(weights)):\n",
    "        result += series.iloc[-n-1] * weights[n]\n",
    "    return float(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weighted_average(ads, [0.6, 0.3, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Экспоненциальное сглаживание\n",
    "\n",
    "А теперь посмотрим, что произойдёт, если вместо взвешивания последних $n$ значений ряда мы начнем взвешивать все доступные наблюдения, при этом экспоненциально уменьшая веса по мере углубления в исторические данные. В этом нам поможет формула простого [экспоненциального сглаживания](http://www.machinelearning.ru/wiki/index.php?title=Экспоненциальное_сглаживание):\n",
    "\n",
    "$$\\hat{y}_{t} = \\alpha \\cdot y_t + (1-\\alpha) \\cdot \\hat y_{t-1} $$\n",
    "\n",
    "Здесь модельное значение представляет собой средневзвешенную между текущим истинным и предыдущим модельным значениями. Вес $\\alpha$ называется сглаживающим фактором. Он определяет, как быстро мы будем \"забывать\" последнее доступное истинное наблюдение. Чем меньше $\\alpha$, тем больше влияния оказывают предыдущие модельные значения, и тем сильнее сглаживается ряд. \n",
    "\n",
    "Экспоненциальность скрывается в рекурсивности функции - каждый раз мы умножаем $(1-\\alpha)$ на предыдущее модельное значение, которое, в свою очередь, также содержало в себе $(1-\\alpha)$, и так до самого начала."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exponential_smoothing(series, alpha):\n",
    "    \"\"\"\n",
    "        series - dataset with timestamps\n",
    "        alpha - float [0.0, 1.0], smoothing parameter\n",
    "    \"\"\"\n",
    "    result = [series[0]] # первое значение такое же, как в исходном ряде\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotExponentialSmoothing(series, alphas):\n",
    "    \"\"\"\n",
    "        Plots exponential smoothing with different alphas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters\n",
    "        \n",
    "    \"\"\"\n",
    "    with plt.style.context('seaborn-white'):    \n",
    "        plt.figure(figsize=(15, 7))\n",
    "        for alpha in alphas:\n",
    "            plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n",
    "        plt.plot(series.values, \"c\", label = \"Actual\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis('tight')\n",
    "        plt.title(\"Exponential Smoothing\")\n",
    "        plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotExponentialSmoothing(ads.Ads, [0.3, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotExponentialSmoothing(currency.GEMS_GEMS_SPENT, [0.3, 0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Двойное экспоненциальное сглаживание\n",
    "\n",
    "До сих пор мы могли получить от наших методов в лучшем случае прогноз лишь на одну точку вперёд (и ещё красиво сгладить ряд), это здорово, но недостаточно, поэтому переходим к расширению экспоненциального сглаживания, которое позволит строить прогноз сразу на две точки вперед (и тоже красиво сглаживать ряд).\n",
    "\n",
    "В этом нам поможет разбиение ряда на две составляющие - уровень (level, intercept) $\\ell$ и тренд $b$ (trend, slope). Уровень, или ожидаемое значение ряда, мы предсказывали при помощи предыдущих методов, а теперь такое же экспоненциальное сглаживание применим к тренду, наивно или не очень полагая, что будущее направление изменения ряда зависит от взвешенных предыдущих изменений.\n",
    "\n",
    "$$\\ell_x = \\alpha y_x + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$$\n",
    "\n",
    "$$b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$$\n",
    "\n",
    "$$\\hat{y}_{x+1} = \\ell_x + b_x$$\n",
    "\n",
    "В результате получаем набор функций. Первая описывает уровень - он, как и прежде, зависит от текущего значения ряда, а второе слагаемое теперь разбивается на предыдущее значение уровня и тренда. Вторая отвечает за тренд - он зависит от изменения уровня на текущем шаге, и от предыдущего значения тренда. Здесь в роли веса в экспоненциальном сглаживании выступает коэффициент $\\beta$. Наконец, итоговое предсказание представляет собой сумму модельных значений уровня и тренда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "    \"\"\"\n",
    "        series - dataset with timeseries\n",
    "        alpha - float [0.0, 1.0], smoothing parameter for level\n",
    "        beta - float [0.0, 1.0], smoothing parameter for trend\n",
    "    \"\"\"\n",
    "    # первое значение такое же, как в исходном ряде\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)+1):\n",
    "        if n == 1:\n",
    "            level, trend = series[0], series[1] - series[0]\n",
    "        if n >= len(series): # прогнозируем\n",
    "            value = result[-1]\n",
    "        else:\n",
    "            value = series[n]\n",
    "        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n",
    "        trend = beta*(level-last_level) + (1-beta)*trend\n",
    "        result.append(level+trend)\n",
    "    return result\n",
    "\n",
    "def plotDoubleExponentialSmoothing(series, alphas, betas):\n",
    "    \"\"\"\n",
    "        Plots double exponential smoothing with different alphas and betas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters for level\n",
    "        betas - list of floats, smoothing parameters for trend\n",
    "    \"\"\"\n",
    "    \n",
    "    with plt.style.context('seaborn-white'):    \n",
    "        plt.figure(figsize=(20, 8))\n",
    "        for alpha in alphas:\n",
    "            for beta in betas:\n",
    "                plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "        plt.plot(series.values, label = \"Actual\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis('tight')\n",
    "        plt.title(\"Double Exponential Smoothing\")\n",
    "        plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotDoubleExponentialSmoothing(ads.Ads, alphas=[0.9, 0.02], betas=[0.9, 0.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotDoubleExponentialSmoothing(currency.GEMS_GEMS_SPENT, alphas=[0.9, 0.02], betas=[0.9, 0.02])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас появилась возможность настраивать уже два параметра - $\\alpha$ и $\\beta$. Первый отвечает за сглаживание ряда вокруг тренда, второй - за сглаживание самого тренда. Чем выше значения, тем больший вес будет отдаваться последним наблюдениям и тем менее сглаженным окажется модельный ряд. Комбинации параметров могут выдавать достаточно причудливые результаты, особенно если задавать их руками. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тройное экспоненциальное сглаживание a.k.a. Holt-Winters\n",
    "\n",
    "Итак, успешно добрались до следующего варианта экспоненциального сглаживания, на сей раз тройного.\n",
    "\n",
    "Идея этого метода заключается в добавлении еще одной, третьей, компоненты - сезонности. Сезонная компонента в модели будет объяснять повторяющиеся колебания вокруг уровня и тренда, а характеризоваться она будет длиной сезона - периодом, после которого начинаются повторения колебаний. Для каждого наблюдения в сезоне формируется своя компонента, например, если длина сезона составляет 7 (например, недельная сезонность), то получим 7 сезонных компонент, по штуке на каждый из дней недели.\n",
    "\n",
    "Получаем новую систему:\n",
    "\n",
    "$$\\ell_x = \\alpha(y_x - s_{x-L}) + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$$\n",
    "\n",
    "$$b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$$\n",
    "\n",
    "$$s_x = \\gamma(y_x - \\ell_x) + (1-\\gamma)s_{x-L}$$\n",
    "\n",
    "$$\\hat{y}_{x+m} = \\ell_x + mb_x + s_{x-L+1+(m-1)modL}$$\n",
    "\n",
    "Уровень теперь зависит от текущего значения ряда за вычетом соответствующей сезонной компоненты, тренд остаётся без изменений, а сезонная компонента зависит от текущего значения ряда за вычетом уровня и от предыдущего значения компоненты. При этом компоненты сглаживаются через все доступные сезоны, например, если это компонента, отвечающая за понедельник, от и усредняться она будет только с другими понедельниками. Подробнее про работу усреднений и оценку начальных значений тренда и сезонных компонент можно почитать [здесь](http://www.itl.nist.gov/div898/handbook/pmc/section4/pmc435.htm). Теперь, имея сезонную компоненту, мы можем предсказывать уже не на один, и даже не на два, а на произвольные $m$ шагов вперёд, что не может не радовать. \n",
    "\n",
    "Ниже импортируем класс для построения модели тройного экспоненциального сглаживания, также известного по фамилиям её создателей - Чарльза Хольта и его студента Питера Винтерса. \n",
    "Дополнительно в модель включен метод Брутлага для построения доверительных интервалов:\n",
    "\n",
    "$$\\hat y_{max_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}+m⋅d_{t−T}$$\n",
    "\n",
    "$$\\hat y_{min_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}-m⋅d_{t−T}$$\n",
    "\n",
    "$$d_t=\\gamma∣y_t−\\hat y_t∣+(1−\\gamma)d_{t−T}$$\n",
    "\n",
    "где $T$ - длина сезона, $d$ - предсказанное отклонение, а остальные параметры берутся из тройного сглаживани. Подробнее о методе и о его применении к поиску аномалий во временных рядах можно прочесть [здесь](https://fedcsis.org/proceedings/2012/pliks/118.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кросс-валидация на временных рядах\n",
    "\n",
    "Перед тем, как построить модель, поговорим, наконец, о не ручной оценке параметров для моделей. \n",
    "\n",
    "Ничего необычного здесь нет, по-прежнему сначала необходимо выбрать подходящуюю для данной задачи функцию потерь, которая будет следить за качеством подгонки модели под исходные данные. Затем будем оценивать на кросс-валидации значение функции потерь при данных параметрах модели, искать градиент, менять в соответствии с ним параметры и бодро опускаться в сторону глобального минимума ошибки. \n",
    "\n",
    "Так как временной ряд внезапно имеет временную структуру, случайно перемешивать в фолдах значения всего ряда без сохранения этой структуры нельзя, иначе в процессе потеряются все взаимосвязи наблюдений друг с другом. Поэтому придется использовать чуть более хитрый способ для оптимизации параметров, официального названия которому я так и не нашел, но на сайте [CrossValidated](https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection), предлагают название \"cross-validation on a rolling basis\", что не дословно можно перевести как кросс-валидация на скользящем окне.\n",
    "\n",
    "Суть достаточно проста - начинаем обучать модель на небольшом отрезке временного ряда, от начала до некоторого $t$, делаем прогноз на $t+n$ шагов вперед и считаем ошибку. Далее расширяем обучающую выборку до $t+n$ значения и прогнозируем с $t+n$ до $t+2*n$, так продолжаем двигать тестовый отрезок ряда до тех пор, пока не упрёмся в последнее доступное наблюдение. В итоге получим столько фолдов, сколько $n$ уместится в промежуток между изначальным обучающим отрезком и всей длиной ряда.\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/f5c/7cd/b39/f5c7cdb39ccd4ba68378ca232d20d864.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, зная, как настраивать кросс-валидацию, подберём с её помощью параметры для модели Хольта-Винтерса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit # здорово, когда всё реализовано до тебя\n",
    "from HoltWinters import HoltWinters                 # код большой, поэтому лежит отдельно\n",
    "\n",
    "def timeseriesCVscore(params, series, loss_function=mean_squared_error, slen=24):\n",
    "    \"\"\"\n",
    "        Returns error on CV  \n",
    "        \n",
    "        params - vector of parameters for optimization\n",
    "        series - dataset with timeseries\n",
    "        slen - season length for Holt-Winters model\n",
    "    \"\"\"\n",
    "    # вектор ошибок\n",
    "    errors = []\n",
    "    \n",
    "    values = series.values\n",
    "    alpha, beta, gamma = params\n",
    "    \n",
    "    # задаём число фолдов для кросс-валидации\n",
    "    tscv = TimeSeriesSplit(n_splits=3) \n",
    "    \n",
    "    # идем по фолдам, на каждом обучаем модель, строим прогноз на отложенной выборке и считаем ошибку\n",
    "    for train, test in tscv.split(values):\n",
    "\n",
    "        model = HoltWinters(series=values[train], slen=slen, \n",
    "                            alpha=alpha, beta=beta, gamma=gamma, n_preds=len(test))\n",
    "        model.triple_exponential_smoothing()\n",
    "        \n",
    "        predictions = model.result[-len(test):]\n",
    "        actual = values[test]\n",
    "        error = loss_function(predictions, actual)\n",
    "        errors.append(error)\n",
    "        \n",
    "    return np.mean(np.array(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data = ads.Ads[:-20] # отложим часть данных для тестирования\n",
    "\n",
    "# инициализируем значения параметров\n",
    "x = [0, 0, 0] \n",
    "\n",
    "# Минимизируем функцию потерь с ограничениями на параметры\n",
    "opt = minimize(timeseriesCVscore, x0=x, \n",
    "               args=(data, mean_squared_log_error), \n",
    "               method=\"TNC\", bounds = ((0, 1), (0, 1), (0, 1))\n",
    "              )\n",
    "\n",
    "# Из оптимизатора берем оптимальное значение параметров\n",
    "alpha_final, beta_final, gamma_final = opt.x\n",
    "print(alpha_final, beta_final, gamma_final)\n",
    "\n",
    "# и подставляем их в модель\n",
    "model = HoltWinters(data, slen = 24, \n",
    "                    alpha = alpha_final, \n",
    "                    beta = beta_final, \n",
    "                    gamma = gamma_final, \n",
    "                    n_preds = 50, scaling_factor = 3)\n",
    "model.triple_exponential_smoothing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotHoltWinters(series, plot_intervals=False, plot_anomalies=False):\n",
    "    \"\"\"\n",
    "        series - dataset with timeseries\n",
    "        plot_intervals - show confidence intervals\n",
    "        plot_anomalies - show anomalies \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.plot(model.result, label = \"Model\")\n",
    "    plt.plot(series.values, label = \"Actual\")\n",
    "    error = mean_absolute_percentage_error(series.values, model.result[:len(series)])\n",
    "    plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n",
    "    \n",
    "    if plot_anomalies:\n",
    "        anomalies = np.array([np.NaN]*len(series))\n",
    "        anomalies[series.values<model.LowerBond[:len(series)]] = \\\n",
    "            series.values[series.values<model.LowerBond[:len(series)]]\n",
    "        anomalies[series.values>model.UpperBond[:len(series)]] = \\\n",
    "            series.values[series.values>model.UpperBond[:len(series)]]\n",
    "        plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n",
    "    \n",
    "    if plot_intervals:\n",
    "        plt.plot(model.UpperBond, \"r--\", alpha=0.5, label = \"Up/Low confidence\")\n",
    "        plt.plot(model.LowerBond, \"r--\", alpha=0.5)\n",
    "        plt.fill_between(x=range(0,len(model.result)), y1=model.UpperBond, \n",
    "                         y2=model.LowerBond, alpha=0.2, color = \"grey\")    \n",
    "        \n",
    "    plt.vlines(len(series), ymin=min(model.LowerBond), ymax=max(model.UpperBond), linestyles='dashed')\n",
    "    plt.axvspan(len(series)-20, len(model.result), alpha=0.3, color='lightgrey')\n",
    "    plt.grid(True)\n",
    "    plt.axis('tight')\n",
    "    plt.legend(loc=\"best\", fontsize=13);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotHoltWinters(ads.Ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotHoltWinters(ads.Ads, plot_intervals=True, plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если посмотреть на смоделированное отклонение, хорошо видно, что модель достаточно резко регирует на значительные изменения в структуре ряда, но при этом быстро возвращает дисперсию к обычным значениям, \"забывая\" прошлое. Такая особенность позволяет неплохо и без значительных затрат на подготовку-обучение модели настроить систему по детектированию аномалий даже в достаточно шумных рядах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "plt.plot(model.PredictedDeviation)\n",
    "plt.grid(True)\n",
    "plt.axis('tight')\n",
    "plt.title(\"Brutlag's predicted deviation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прогоним тот же самый алгоритм для второго временного ряда, в котором, как мы знаем, есть тренд и месячная сезонность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data = currency.GEMS_GEMS_SPENT[:-50] # отложим часть данных для тестирования\n",
    "slen = 30 # месячная сезонность\n",
    "# инициализируем значения параметров\n",
    "x = [0, 0, 0] \n",
    "\n",
    "# Минимизируем функцию потерь с ограничениями на параметры\n",
    "opt = minimize(timeseriesCVscore, x0=x, \n",
    "               args=(data, mean_absolute_percentage_error, slen), \n",
    "               method=\"TNC\", bounds = ((0, 1), (0, 1), (0, 1))\n",
    "              )\n",
    "\n",
    "# Из оптимизатора берем оптимальное значение параметров\n",
    "alpha_final, beta_final, gamma_final = opt.x\n",
    "print(alpha_final, beta_final, gamma_final)\n",
    "\n",
    "\n",
    "model = HoltWinters(data, slen = slen, \n",
    "                    alpha = alpha_final, \n",
    "                    beta = beta_final, \n",
    "                    gamma = gamma_final, \n",
    "                    n_preds = 100, scaling_factor = 3)\n",
    "model.triple_exponential_smoothing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotHoltWinters(currency.GEMS_GEMS_SPENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит весьма правдоподобно, модель уловила и восходящий тренд, и сезонные пики, и в целом достаточно точно описывает наш временной ряд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotHoltWinters(currency.GEMS_GEMS_SPENT, plot_intervals=True, plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(model.PredictedDeviation)\n",
    "plt.grid(True)\n",
    "plt.axis('tight')\n",
    "plt.title(\"Brutlag's predicted deviation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Эконометрический подход"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стационарность\n",
    "\n",
    "Перед тем, как перейти к моделированию, стоит сказать о таком важном свойстве временного ряда, как [**стационарность**](https://ru.wikipedia.org/wiki/Стационарность). \n",
    "Под стационарностью понимают свойство процесса не менять своих статистических характеристик с течением времени, а именно постоянство матожидания, постоянство дисперсии (она же [гомоскедастичность](https://ru.wikipedia.org/wiki/Гомоскедастичность)) и независимость ковариационной функции от времени (должна зависеть только от расстояния между наблюдениями). Наглядно можно посмотреть на эти свойства на картинках, взятых из поста [Sean Abu](http://www.seanabu.com/2016/03/22/time-series-seasonal-ARIMA-model-in-python/):\n",
    "\n",
    "- Временной ряд справа не является стационарным, так как его матожидание со временем растёт\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/20c/9d8/a63/20c9d8a633ec436f91dccd4aedcc6940.png\"/>\n",
    "\n",
    "- Здесь не повезло с дисперсией - разброс значений ряда существенно варьируется в зависимости от периода\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/b88/eec/a67/b88eeca676d642449cab135273fd5a95.png\"/>\n",
    "\n",
    "- Наконец, на последнем графике видно, что значения ряда внезапно становятся ближе друг ко другу, образуя некоторый кластер, а в результате получаем непостоянство ковариаций\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/2f6/1ee/cb2/2f61eecb20714352840748b826e38680.png\"/>\n",
    "\n",
    "Почему стационарность так важна? По стационарному ряду просто строить прогноз, так как мы полагаем, что его будущие статистические характеристики не будут отличаться от наблюдаемых текущих. Большинство моделей временных рядов так или иначе моделируют и предсказывают эти характеристики (например, матожидание или дисперсию), поэтому в случае нестационарности исходного ряда предсказания окажутся неверными. К сожалению, большинство временных рядов, с которыми приходится сталкиваться за пределыми учебных материалов, стационарными не являются, но с этим можно (и нужно) бороться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Избавляемся от нестационарности и строим SARIMA\n",
    "\n",
    "Попробуем теперь построить SARIMA модель, пройдя все ~~круги ада~~ стадии приведения ряда к стационарному виду. Подробно про саму модель можно почитать здесь - [Построение модели SARIMA с помощью Python+R](https://habrahabr.ru/post/210530/), [Анализ временных рядов с помощью python](https://habrahabr.ru/post/207160/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tsplot(y, lags=None, figsize=(12, 7), style='bmh'):\n",
    "    \"\"\"\n",
    "        Plot time series, its ACF and PACF, calculate Dickey–Fuller test\n",
    "        \n",
    "        y - timeseries\n",
    "        lags - how many lags to include in ACF, PACF calculation\n",
    "    \"\"\"\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "        \n",
    "    with plt.style.context(style):    \n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        layout = (2, 2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "        \n",
    "        y.plot(ax=ts_ax)\n",
    "        ts_ax.set_title('Time Series Analysis Plots')\n",
    "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "\n",
    "        print(\"Критерий Дики-Фуллера: p=%f\" % sm.tsa.stattools.adfuller(y)[1])\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsplot(ads.Ads, lags=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, этот ряд по критерию Дикки-Фуллера у нас стационарен, по его графику тоже видно, что тренд, как таковой, отсутствует, т.е., матожидание постоянно, разброс вокруг среднего тоже примерно одинаковый, значит, постоянство дисперсии также наблюдается. Остаётся сезонность, с которой нужно разобраться перед построением модели.\n",
    "Для этого выполним преобразование под хитрым названием \"сезонное дифференцирование\", под которым скрывается простое вычитание из ряда самого себя с лагом равным периоду сезонности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ads_diff = ads.Ads - ads.Ads.shift(24)\n",
    "tsplot(ads_diff[24:], lags=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже лучше, от видимой сезонности избавились, в автокорреляционной функции по-прежнему осталось много значимых лагов, попробуем от них избавиться, взяв еще и первые разности - то есть вычитание ряда самого из себя с лагом в один шаг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ads_diff = ads_diff - ads_diff.shift(1)\n",
    "tsplot(ads_diff[24+1:], lags=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, теперь ряд выглядит как непонятно что, колеблющееся вокруг нуля, по критерию Дикки-Фуллера он стационарен, а в автокорреляционной функции пропали многочисленные значимые пики. Можно приступать к моделированию! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Экспресс-введение в ARIMA-семейство\n",
    "\n",
    "Cовсем вкратце о самой модели. Начнём по буквам составлять это длинное название $SARIMA(p, d, q)(P, D, Q, s)$, Seasonal Autoregression Moving Average model:\n",
    "\n",
    "- $AR(p)$ - модель авторегрессии, т.е. регрессии временного ряда на самого себя, в основе лежит предположение о том, что временной ряд зависит от своих предыдущих значений с некоторым лагом (или с несколькими лагами), максимальный лаг, который входит в модель, обозначается буквой $p$. Определить начальное значение $p$ можно при помощи графика PACF по значимому лагу, после которого большинство оставшихся лагов становятся не значимыми.\n",
    "- $MA(q)$ - модель скользящего среднего, если не вдаваться в детали - моделирует случайные ошибки временного ряда, т.е., как текущая ошибка во временном ряду зависит от предыдущих с некоторым лагом, здесь лаг обозначается буквой $q$. Начальное значение можно определить по графику ACF, логика аналогична.\n",
    "\n",
    "Немножко остановимся и объединим первые четыре буквы, чтобы посмотреть, что у нас получилось:\n",
    "\n",
    "$AR(p) + MA(q) = ARMA(p, q)$\n",
    "\n",
    "А у нас получилась модель авторегрессии - скользящего среднего, если временной ряд стационарен, его можно смело аппроксимировать этими четырьмя буквами. Продолжим-с\n",
    "\n",
    "- $I(d)$ - порядок интегрированности временного ряда. За этими словами стоит очень простая штука - сколько нам обычных разностей в ряде пришлось взять, чтобы привести его к стационарному, такой порядок интегрированности и будет. В нашем случае это единичка, так как мы взяли от нашего ряда первую разность. \n",
    "\n",
    "Добавим к нашим четырём буквам и получим модель $ARIMA$, которая умеет работать с нестационарными временными рядами, которые можно привести к стационарным при помощи взятия разностей некоторого порядка. Отлично, осталась последняя буква!\n",
    "\n",
    "- $S(s)$ - эта буква отвечает за сезонность и равна длине периода сезонности во временном ряде\n",
    "\n",
    "Прикрепляем последнюю букву и оказывается, что вместо одного дополнительного параметра у нас добавилось сразу три - $(P, D, Q)$\n",
    "\n",
    "- $P$ - порядок модели авторегрессии для сезонной компоненты, также определяется по PACF, только смотреть нужно на число значимых лагов, кратных периоду сезонности, например, если период равен 24 и выясняется, что по PACF значим 24-й и 48-й лаг, то для начального приближения можно взять $P = 2$\n",
    "\n",
    "- $Q$ - аналогично, но для модели скользящего среднего по сезонной компоненте, определяется по ACF\n",
    "\n",
    "- $D$ - порядок сезонной интегрированности временного ряда. Может быть равен либо 0, либо 1, так как сезонное дифференцирование либо проводилось, либо нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы знаем, как подбирать параметры, еще раз посмотрим на график получившегося после преобразований стационарного ряда и попробуем подобрать начальные приближения параметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsplot(ads_diff[24+1:], lags=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# p = \n",
    "# d = \n",
    "# q = \n",
    "\n",
    "# P = \n",
    "# D = \n",
    "# Q = \n",
    "# s = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# зададим ограничения на возможные значения параметров для перебора\n",
    "ps = range(2, 5)\n",
    "d=1 # тут мы уверены, дифференцировали один раз\n",
    "qs = range(2, 5)\n",
    "\n",
    "Ps = range(0, 2)\n",
    "D=1 # здесь тоже уверены, сезонное дифференцирование было одно\n",
    "Qs = range(0, 2)\n",
    "s = 24\n",
    "\n",
    "# создадим лист с возможными параметрами и посчитаем его длину\n",
    "parameters = product(ps, qs, Ps, Qs)\n",
    "parameters_list = list(parameters)\n",
    "len(parameters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizeSARIMA(parameters_list, d, D, s):\n",
    "    \"\"\"\n",
    "        Return dataframe with parameters and corresponding AIC\n",
    "        \n",
    "        parameters_list - list with (p, q, P, Q) tuples\n",
    "        d - integration order in ARIMA model\n",
    "        D - seasonal integration order \n",
    "        s - length of season\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    best_aic = float(\"inf\")\n",
    "\n",
    "    for param in tqdm_notebook(parameters_list):\n",
    "        # try except нужен, потому что на некоторых наборах параметров модель не обучается\n",
    "        try:\n",
    "            model=sm.tsa.statespace.SARIMAX(ads.Ads, order=(param[0], d, param[1]), \n",
    "                                            seasonal_order=(param[3], D, param[3], s)).fit(disp=-1)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        aic = model.aic\n",
    "        #сохраняем лучшую модель, aic, параметры\n",
    "        if aic < best_aic:\n",
    "            best_model = model\n",
    "            best_aic = aic\n",
    "            best_param = param\n",
    "        results.append([param, model.aic])\n",
    "\n",
    "    result_table = pd.DataFrame(results)\n",
    "    result_table.columns = ['parameters', 'aic']\n",
    "    # сортируем по возрастанию, чем меньше критерий AIC, тем лучше\n",
    "    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    return result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result_table = optimizeSARIMA(parameters_list, d, D, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# передаём параметры с минимальным AIC\n",
    "p, q, P, Q = result_table.parameters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# обучаем лучшую модель\n",
    "best_model=sm.tsa.statespace.SARIMAX(ads.Ads, order=(p, d, q), \n",
    "                                        seasonal_order=(P, D, Q, s)).fit(disp=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(best_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsplot(best_model.resid[24+1:], lags=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что ж, остатки стационарны, явных автокорреляций нет, построим прогноз по получившейся модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotSARIMA(series, model, n_steps):\n",
    "    \"\"\"\n",
    "        Plots model vs predicted values\n",
    "        \n",
    "        series - dataset with timeseries\n",
    "        model - fitted SARIMA model\n",
    "        n_steps - number of steps to predict in the future\n",
    "        \n",
    "    \"\"\"\n",
    "    # подставляем модельные значения\n",
    "    data = series.copy()\n",
    "    data.columns = ['actual']\n",
    "    data['arima_model'] = model.fittedvalues\n",
    "    # делаем отступ на s+d, так как эти значения в модели были ненаблюдаемыми\n",
    "    # из-за дифференцирования\n",
    "    data['arima_model'][:s+d] = np.NaN\n",
    "    \n",
    "    # прогнозируем на n_steps шагов вперёд \n",
    "    forecast = model.predict(start = data.shape[0], end = data.shape[0]+n_steps)\n",
    "    forecast = data.arima_model.append(forecast)\n",
    "    # считаем ошибку, также отступив s+d шагов от начала\n",
    "    error = mean_absolute_percentage_error(data['actual'][s+d:], data['arima_model'][s+d:])\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n",
    "    plt.plot(forecast, color='r', label=\"model\")\n",
    "    plt.axvspan(data.index[-1], forecast.index[-1], alpha=0.5, color='lightgrey')\n",
    "    plt.plot(data.actual, label=\"actual\")\n",
    "    plt.legend()\n",
    "    plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotSARIMA(ads, best_model, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В финале получаем очень хороший прогноз, заказчику средняя абсолютная ошибка в 4 процента также наверняка понравится, однако суммарные затраты на подготовку данных, приведение к стационарности, определение и перебор параметров могут такой точности и не стоить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "В качестве домашнего задания и хорошей практики для закрепления - по аналогии построить модель SARIMA для прогнозирования второго временного ряда, имеющегося в нашем распоряжении, в котором присутствует тренд (убирается при помощи первых разностей) и месячная сезонность (сезонное дифференцирование с периодом 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsplot(currency.GEMS_GEMS_SPENT, lags=60)\n",
    "\n",
    "# TODO\n",
    "# - привести ряд к стационарному виду\n",
    "# - по ACF, PACF определить начальные значения параметров модели и возможные интервалы\n",
    "# - перебором параметров подобрать оптимальную модель и построить её\n",
    "# - построить график модельных значений и остатков, убедиться в адекватности результатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Machine Learning для временных рядов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Временные ряды без правил или прогнозируем, чем можем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Снова небольшое лирическое отступление. Часто на работе приходится строить модели, руководствуясь одним основополагающим принципом – [быстро, качественно, недорого](http://lurkmore.to/%D0%91%D1%8B%D1%81%D1%82%D1%80%D0%BE,_%D0%BA%D0%B0%D1%87%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE,_%D0%BD%D0%B5%D0%B4%D0%BE%D1%80%D0%BE%D0%B3%D0%BE). Поэтому часть моделей могут банально не подойти для \"продакшн-решений\", так как либо требуют слишком больших затрат по подготовке данных, либо сложно настраиваются, либо требуют частого переобучения на новых данных, поэтому зачастую гораздо проще бывает выделить несколько признаков из имеющегося временного ряда и построить по ним обычную линейную регрессию или навесить решаюший лес. Дешево и сердито.\n",
    "\n",
    "\n",
    "Возможно, этот подход не является значительно подкрепленным теорией, нарушает различные предпосылки, например, условия Гаусса-Маркова, особенно пункт про некоррелированность ошибок, однако на практике нередко выручает и достаточно активно используется в соревнованиях по машинному обучению."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Извлечение признаков (Feature exctraction)\n",
    "\n",
    "Итак, для модели нужны признаки, а у нас есть только одномерный временной ряд, что можно придумать?\n",
    "\n",
    "* Конечно же, лаги временного ряда\n",
    "* Статистики по окнам:\n",
    "    - Минимальное/максимальное значение ряда в окне\n",
    "    - Среднеё/медианное значение по окну\n",
    "    - Дисперсия по окну\n",
    "    - и т.д.\n",
    "* Признаки из самой даты и времени:\n",
    "    - Минута часа, час дня, день недели, месяц года и т.д.\n",
    "    - Является ли день праздником/выходным/специфическим днём\n",
    "* Кодирование средним\n",
    "* Прогнозы других моделей (но тут в скорости можно потерять)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пройдемся по некоторым из способов и посмотрим, что мы можем вытащить из нашего временного ряда с рекламой\n",
    "\n",
    "- Лаги временного ряда\n",
    "\n",
    "Сдвигая ряд на $n$ шагов мы получаем столбец-признак, в котором текущему значению ряда в момент $t$ будет соответствовать его значение в момент времени $t-n$. Таким образом, если сделать отступ в 1 шаг, то модель, обученная на таком признаке, будет способна давать предсказание на 1 шаг вперед, зная текущее состояние ряда. Увеличивая сдвиг, например, до 6 лага позволит модели делать предсказания на 6 шагов вперёд, однако опираться она будет на данные, которые видела 6 временных периодов назад и если за это время что-то кардинально поменялось, модель сразу не уловит изменений и выдаст прогноз с большой ошибкой. Поэтому при выборе начального лага приходиться балансировать между желанием получить предсказания на бОльшее число периодов вперёд и приемлимым качеством предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Создадим копию исходного датафрейма, чтобы можно было выполнять различные преобразования\n",
    "data = pd.DataFrame(ads.Ads.copy())\n",
    "data.columns = [\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Добавляем лаги целевой переменной\n",
    "for i in range(6, 25):\n",
    "    data[\"lag_{}\".format(i)] = data.y.shift(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# посмотрим, как теперь выглядит датасет\n",
    "data.tail(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шикарно, у нас есть датасет, почему бы не построить модель!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# для рассчета ошибки на кросс-валидации будем бить ряд на 5 фолдов\n",
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeseries_train_test_split(X, y, test_size):\n",
    "    \"\"\"\n",
    "        Perform train-test split with respect to time series structure\n",
    "    \"\"\"\n",
    "    \n",
    "    # считаем индекс в датафрейме, после которого начинается тестовый отрезок\n",
    "    test_index = int(len(X)*(1-test_size))\n",
    "    \n",
    "    # разбиваем весь датасет на тренировочную и тестовую выборку\n",
    "    X_train = X.iloc[:test_index]\n",
    "    y_train = y.iloc[:test_index]\n",
    "    X_test = X.iloc[test_index:]\n",
    "    y_test = y.iloc[test_index:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data.dropna().y\n",
    "X = data.dropna().drop(['y'], axis=1)\n",
    "\n",
    "# разбиваем датасет на тренировочную и тестовую часть, отложим 30% данных\n",
    "X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# учим регрессию\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotModelResults(model, X_train=X_train, X_test=X_test, plot_intervals=False, plot_anomalies=False):\n",
    "    \"\"\"\n",
    "    Строит график прогнозных и фактических значений, а также доверительных интервалов прогноза и аномалий\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # получаем предсказания по модели\n",
    "    prediction = model.predict(X_test)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n",
    "    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n",
    "    \n",
    "    if plot_intervals:\n",
    "        cv = cross_val_score(model, X_train, y_train, \n",
    "                                    cv=tscv, \n",
    "                                    scoring=\"neg_mean_absolute_error\")\n",
    "        mae = cv.mean() * (-1)\n",
    "        deviation = cv.std()\n",
    "        \n",
    "        scale = 1.96\n",
    "        lower = prediction - (mae + scale * deviation)\n",
    "        upper = prediction + (mae + scale * deviation)\n",
    "        \n",
    "        plt.plot(lower, \"r--\", label=\"upper bond / lower bond\", alpha=0.5)\n",
    "        plt.plot(upper, \"r--\", alpha=0.5)\n",
    "        \n",
    "        if plot_anomalies:\n",
    "            anomalies = np.array([np.NaN]*len(y_test))\n",
    "            anomalies[y_test<lower] = y_test[y_test<lower]\n",
    "            anomalies[y_test>upper] = y_test[y_test>upper]\n",
    "            plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n",
    "    \n",
    "    error = mean_absolute_percentage_error(prediction, y_test)\n",
    "    plt.title(\"Mean absolute percentage error {0:.2f}%\".format(error))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True);\n",
    "    \n",
    "def plotCoefficients(model):\n",
    "    \"\"\"\n",
    "    Отрисовывает отсортированные по абсолютному значению коэффициенты модели\n",
    "    \"\"\"\n",
    "    \n",
    "    coefs = pd.DataFrame(model.coef_, X_train.columns)\n",
    "    coefs.columns = [\"coef\"]\n",
    "    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n",
    "    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    coefs.coef.plot(kind='bar')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotModelResults(lr, plot_intervals=True)\n",
    "plotCoefficients(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что ж, всего лишь простыми лагами удалось построить модель, которая не сильно уступает по качеству SARIMA модели. Пока что в модели есть достаточно много мусорных факторов, но отбором займемся чуть позже, а пока продолжим придумывать признаки!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим в датасет час, день недели и выходной в качестве категориальных переменных. Для этого переводим имеющийся в датафрейме индекс в формат `datetime`, и извлекаем из него `hour` и `weekday`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.index = data.index.to_datetime()\n",
    "data[\"hour\"] = data.index.hour\n",
    "data[\"weekday\"] = data.index.weekday\n",
    "data['is_weekend'] = data.weekday.isin([5,6])*1\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно визуализировать получившиеся признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "plt.title(\"Закодированные переменные\")\n",
    "data.hour.plot()\n",
    "data.weekday.plot()\n",
    "data.is_weekend.plot()\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как теперь у нас данные в разном масштабе - десятки тысяч в лагах и единицы в категориях, имеет смысл привести всё к одному масштабу, чтобы можно было по-прежнему исследовать коэффициенты модели. Для этого воспользуемся `StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data.dropna().y\n",
    "X = data.dropna().drop(['y'], axis=1)\n",
    "\n",
    "# разбиваем датасет на тренировочную и тестовую часть, отложим 30% данных\n",
    "X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# учим регрессию\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True)\n",
    "plotCoefficients(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибка на тесте еще немного уменьшилась, и по коэффициентам видно, что день недели и выходной - довольно неплохие признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Кодирование средним**\n",
    "\n",
    "Иногда в качестве признака добавлять среднее значение целевой переменной. В нашем случае каждый день недели или час дня можно закодировать сооветствующим средним числом просмотров рекламы/траты инровой валюты, пришедшиеся на этот день недели или этот час. При этом важно следить за тем, чтобы расчет среднего значения производился только в рамках тренировочного датасета (или в рамках текущего наблюдаемого фолда при кросс-валидации), иначе можно ненароком привнести в модель информацию о будущем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def code_mean(data, cat_feature, real_feature):\n",
    "    \"\"\"\n",
    "    Возвращает словарь, где ключами являются уникальные категории признака cat_feature, \n",
    "    а значениями - средние по real_feature\n",
    "    \"\"\"\n",
    "    return dict(data.groupby(cat_feature)[real_feature].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на средние по часам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_hour = code_mean(data, 'hour', \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "plt.title(\"Средние значения ряда по часам\")\n",
    "pd.DataFrame.from_dict(average_hour, orient='index')[0].plot()\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь, объединим все преобразования под одной функцией и добавим к признакам кодирование средним"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareData(series, lag_start, lag_end, test_size, target_encoding=False):\n",
    "    \"\"\"\n",
    "        series: pd.DataFrame\n",
    "            dataframe with timeseries\n",
    "\n",
    "        lag_start: int\n",
    "            initial step back in time to slice target variable \n",
    "            example - lag_start = 1 means that the model \n",
    "                      will see yesterday's values to predict today\n",
    "\n",
    "        lag_end: int\n",
    "            final step back in time to slice target variable\n",
    "            example - lag_end = 4 means that the model \n",
    "                      will see up to 4 days back in time to predict today\n",
    "\n",
    "        test_size: float\n",
    "            size of the test dataset after train/test split as percentage of dataset\n",
    "\n",
    "        target_encoding: boolean\n",
    "            if True - add target averages to the dataset\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Создадим копию исходного датафрейма, чтобы можно было выполнять различные преобразования\n",
    "    data = pd.DataFrame(series.copy())\n",
    "    data.columns = [\"y\"]\n",
    "    \n",
    "    # Добавляем лаги целевой переменной\n",
    "    for i in range(lag_start, lag_end):\n",
    "        data[\"lag_{}\".format(i)] = data.y.shift(i)\n",
    "    \n",
    "    # Добавляем данные по часу, дню недели и выходным\n",
    "    data.index = data.index.to_datetime()\n",
    "    data[\"hour\"] = data.index.hour\n",
    "    data[\"weekday\"] = data.index.weekday\n",
    "    data['is_weekend'] = data.weekday.isin([5,6])*1\n",
    "    \n",
    "    if target_encoding:\n",
    "        # считаем средние только по тренировочной части, чтобы избежать лика\n",
    "        test_index = int(len(data.dropna())*(1-test_size))\n",
    "        data['weekday_average'] = list(map(code_mean(data[:test_index], 'weekday', \"y\").get, data.weekday))\n",
    "        data[\"hour_average\"] = list(map(code_mean(data[:test_index], 'hour', \"y\").get, data.hour))\n",
    "\n",
    "        # выкидываем закодированные средними признаки \n",
    "        data.drop([\"hour\", \"weekday\"], axis=1, inplace=True)\n",
    "    \n",
    "    # Делим на тренировочную и тестовую\n",
    "    y = data.dropna().y\n",
    "    X = data.dropna().drop(['y'], axis=1)\n",
    "    X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =\\\n",
    "prepareData(ads.Ads, lag_start=6, lag_end=25, test_size=0.3, target_encoding=True)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# учим регрессию\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True, plot_anomalies=True)\n",
    "plotCoefficients(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот и немного переобучения, переменная hour_average настолько понравилась нашей модели на тренировочном датасете, что по ней, в основном, модель и стала прогнозировать. Решить эту проблему можно по-разному, например, считать средние таргета не по всему тренировочному датасету, а по некоторому последнему окну, которое будет лучше отражать последующее состояние ряда. Можно и просто руками выкинуть эту переменную, раз мы уверены, что она ведёт к ухудшению качества, что мы сейчас и сделаем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =\\\n",
    "prepareData(ads.Ads, lag_start=6, lag_end=25, test_size=0.3, target_encoding=False)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризация и отбор признаков\n",
    "\n",
    "Как мы уже выяснили, не все признаки одинаково полезны, а некоторые даже вредны и от них нужно срочно избавляться. Помимо выкидывания руками некоторых признаков, можно заняться регуляризацией. Сейчас мы рассмотрим две наиболее распространенных модели с регуляризацией - ридж и лассо регрессии. Они добавляют к нашей функции потерь, при помощи которой мы оптимизируем значения коэффициентов модели, еще немножко ограничений. \n",
    "\n",
    "В случае с ридж регрессией - это сумма квадратов коэффициентов модели, помноженная на коэффициент регуляризации. Т.е. чем больше значение коэффициента у переменной, тем больше будет наша ошибка, а значит, мы будем стараться оптимизировать одновременно как подгонку модели под данные, так и не давать коэффициентам сильно разрастаться. \n",
    "\n",
    "В результате такой регуляризации, которая носит гордое название $L2$, мы получаем бОльшее смещение, но меньший разброс, т.е. модель будет (по крайней мере мы на это очень надеемся) обладать лучшей обобщающей способностью. \n",
    "\n",
    "Вторая модель - лассо регрессия, в ней к функционалу ошибки добавляются не квадраты, а модули коэффициентов, в результате чего во время оптимизации коэффициенты перед факторами, которые оказываются не так важны, просто зануляются. Такой тип регуляризации называется $L1$. Таким образом, лассо регрессия автоматически позволяет производить отбор факторов для модели, сокращая признаковое пространство."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала убедимся, что нам есть, что отбрасывать и в данных действительно очень много скоррелированных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(X_train.corr());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge = RidgeCV(cv=tscv)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResults(ridge, \n",
    "                 X_train=X_train_scaled, \n",
    "                 X_test=X_test_scaled, \n",
    "                 plot_intervals=True, plot_anomalies=True)\n",
    "plotCoefficients(ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошо видно, как всё ближе и ближе прижимаются к нулю (хотя никогда нулю не равны) коэффициенты в модели по мере убывания их важности. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lasso = LassoCV(cv=tscv)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResults(lasso, \n",
    "                 X_train=X_train_scaled, \n",
    "                 X_test=X_test_scaled, \n",
    "                 plot_intervals=True, plot_anomalies=True)\n",
    "plotCoefficients(lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лассо регрессия оказалась более консервативой и убрала 23-й лаг из важных факторов модели, что, в результате, только пошло на пользу качеству на отложенной выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Почему бы теперь не попробовать XGBoost\n",
    "<img src=\"https://habrastorage.org/files/754/a9a/26e/754a9a26e59648de9fe2487241a27c43.jpg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/Users/dmitrys/xgboost/python-package/')\n",
    "from xgboost import XGBRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotModelResults(xgb, \n",
    "                 X_train=X_train_scaled, \n",
    "                 X_test=X_test_scaled, \n",
    "                 plot_intervals=True, plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот и самая маленькая ошибка на отложенной выборке из всех опробованных, xgboost победил! \n",
    "\n",
    "Однако эта победа обманчива и не стоит первым делом при работе с временным рядом расчехлять xgboost. Дело в том, что модели с деревьями внутри, в отличие от линейных, довольно плохо справляются с трендами без помощи трюков с обработкой данных. В идеале - предварительно очистить данные от тренда, спрогнозировать тренд отдельно при помощи линейной модели, и отдельно - очищенный ряд, а потом сложить вместе предсказания. \n",
    "\n",
    "Снова небольшим домашним заданием будет построение моделей для второго временного ряда, с внутриигровой валютой.\n",
    "\n",
    "TODO\n",
    "\n",
    "- Изменить функцию `prepareData` так, чтобы она не извлекала признак \"час\" и \"среднее по часу\", а вместо этого добавить извлечение признака \"день месяца\" и \"среднее по дню месяца\"\n",
    "- Подготовить данные для построения модели, взяв `lag_start = 7`, чтобы модель могла прогнозировать на неделю вперед, а `lag_end = 31`, чтобы месячная сезонность также попала в наблюдаемые моделью значения\n",
    "- Построить линейную модель с `target_encoding` (среднее по дню месяца) и без, проверить, что лучше\n",
    "- Построить ридж и лассо регрессии, посмотреть, какие факторы модели считают наиболее важными\n",
    "- Забустить этот временной ряд и убедиться, что по умолчанию `xgb` будет давать отвратительный результат\n",
    "\n",
    "Крутое бонус-задание\n",
    "- Разбейте подготовленные данные на `train` и `test`\n",
    "- Очистите временной ряд от линейного тренда при помощи построения однофакторной линейной регрессии, где таргетом будут значения ряда, а фактором - числовой ряд от 0 до `len(train)`. Предсказанные по такой модели значения будут линейным трендом, который нужно вычесть из исходного ряда (подбробнее можно посмотреть [здесь](https://machinelearningmastery.com/time-series-trends-in-python/))\n",
    "- Теперь спрогнозируйте значения тренда по этой модели для тестового датасета, для этого нужно в качестве фактора взять числовой ряд от `len(train)` до `len(test)`, очистите тестовый датасет от тренда при помощи этого прогноза\n",
    "- Далее на очищенных от тренда данных `train` постройте xgboost и постройте прогноз на `test`\n",
    "- Наконец, просуммируйте предсказанное значение тренда и прогнозы бустинга, чтобы получить красивый итоговый прогноз временного ряда"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) To Infinity and Beyond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook Prophet - автоматические прогнозы\n",
    "https://github.com/facebook/prophet\n",
    "\n",
    "[Предсказываем будущее с помощью библиотеки Facebook Prophet](https://habrahabr.ru/company/ods/blog/323730/)\n",
    "\n",
    "Библиотека prophet позволяет быстро и достаточно качественно автоматически строить прогнозы временныз рядов. Подробно про теорию работы можно почитать в замечательной статьей по ссылке выше, а сейчас посмотрим на практике, как она работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# задаем число предсказаний будущих значений\n",
    "predictions = 60\n",
    "\n",
    "# создаем специфический датасет, с определенными названиями столбцов\n",
    "df = ads.reset_index()\n",
    "df.columns = ['ds', 'y']\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# откладываем часть данных для тестирования\n",
    "train_df = df[:-predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# обучаем модель, обязательно указывая тип сезонности, который хотим использовать\n",
    "model = Prophet(weekly_seasonality=True)\n",
    "model.fit(train_df)\n",
    "\n",
    "# создаем еще один специфический датафрейм, который нужен для предсказаний\n",
    "# здесь нужно указать частоту данных (по умолчанию - дни)\n",
    "future = model.make_future_dataframe(periods=predictions, freq='H')\n",
    "future.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# делаем предсказания \n",
    "forecast = model.predict(future)\n",
    "forecast.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# а теперь рисуем картинки\n",
    "error = mean_absolute_percentage_error(df[-predictions:]['y'], forecast[-predictions:]['yhat'])\n",
    "print(\"Mean absolute percentage error {0:.2f}%\".format(error))\n",
    "model.plot(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно посмотреть на декомпозицию исходного ряда на составляющие"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras\n",
    "\n",
    "Наконец, последний штрих, правда, уже без кода и примеров - с временными рядами можно работать и используя нейронные сети, в частности, RNN и, чуть более разумно, LSTM. Подробнее можно ознакомиться в [этой статьей](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/). Однако по сравнению с настройкой нейронной сети для работы с одним временным рядом даже полный цикл подготовки и обучения SARIMA, на мой взгляд, будет в разы быстрее и, главное, качественнее."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
